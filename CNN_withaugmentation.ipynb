{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN Model for American Sign Language Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One: Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = {\n",
    "    '0':1, '1':2, '2':3, '3':4, '4':5, '5':6, '6':7, '7':8, '8':9, '9':10,\n",
    "    'a':11, 'b':12, 'c':13, 'd':14, 'e':15, 'f':16, 'g':17, 'h':18, 'i':19,\n",
    "    'j':20, 'k':21, 'l':22, 'm':23, 'n':24, 'o':25, 'p':26, 'q':27, 'r':28,\n",
    "    's':29, 't':30, 'u':31, 'v':32, 'w':33, 'x':34, 'y':35, 'z':36, \n",
    "}\n",
    "img_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "path = 'Data/asl/train_asl/'\n",
    "gestures = os.listdir(path)[1:]\n",
    "\n",
    "for ix in gestures:\n",
    "    images = os.listdir(path +ix)\n",
    "    for cx in images:\n",
    "        img_path = path+'/' +ix + '/' + cx\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.resize(img, (img_size,img_size), interpolation = cv2.INTER_AREA) \n",
    "        \n",
    "        img = img.reshape((img_size, img_size,3))\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "        transformed_img = datagen.flow(img, batch_size=1)\n",
    "        for i in range(2):\n",
    "            image = next(transformed_img)[0].astype('uint8')\n",
    "            x.append(image)\n",
    "            y.append(dict_labels[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(np.array(y))\n",
    "X = np.array(x)/255.0 # normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "x_test = []\n",
    "y_test = []\n",
    "path_test = 'Data/asl/test_asl/'\n",
    "gestures_test = os.listdir(path_test)[1:]\n",
    "\n",
    "for ix in gestures_test:\n",
    "    images = os.listdir(path_test +ix)\n",
    "    for cx in images:\n",
    "        img_path = path_test+'/' +ix + '/' + cx\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.resize(img, (img_size,img_size), interpolation = cv2.INTER_AREA) \n",
    "\n",
    "        x_test.append(img)\n",
    "        y_test.append(dict_labels[ix])\n",
    "        \n",
    "Y_test = to_categorical(np.array(y_test))\n",
    "X_test = np.array(x_test)/255.0   # normalization     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data into train set and validation set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels_new = {\n",
    "    '0':1, '1':2, '2':3, '3':4, '4':5, '5':6, '6':7, '7':8, '8':9, '9':10,\n",
    "    'A':11, 'B':12, 'C':13, 'D':14, 'E':15, 'F':16, 'G':17, 'H':18, 'I':19,\n",
    "    'J':20, 'K':21, 'L':22, 'M':23, 'N':24, 'O':25, 'P':26, 'Q':27, 'R':28,\n",
    "    'S':29, 'T':30, 'U':31, 'V':32, 'W':33, 'X':34, 'Y':35, 'Z':36, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "path = 'Gesture/GestureImageData/'\n",
    "gestures = os.listdir(path)[1:]\n",
    "\n",
    "\n",
    "for ix in gestures:\n",
    "    if(ix!='_'):\n",
    "        images = os.listdir(path +ix)\n",
    "        for cx in images:\n",
    "            img_path = path+'/' +ix + '/' + cx\n",
    "            img = cv2.imread(img_path, 1)\n",
    "            img = cv2.resize(img, (img_size,img_size), interpolation = cv2.INTER_AREA) \n",
    "\n",
    "            img = img.reshape((img_size, img_size,3))\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "        \n",
    "            datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "            transformed_img = datagen.flow(img, batch_size=1)\n",
    "            for i in range(2):\n",
    "                image = next(transformed_img)[0].astype('uint8')\n",
    "                xs.append(image)\n",
    "                ys.append(dict_labels_new[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys_temp = to_categorical(np.array(ys))\n",
    "Xs_temp = np.array(xs)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Xs_test, Ys, Ys_test = train_test_split(Xs_temp, Ys_temp, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train, Xs_val, Ys_train, Ys_val = train_test_split(Xs, Ys, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_merged = tf.concat([X_train, Xs_train], axis=0)\n",
    "x_test_merged = tf.concat([X_test, Xs_test], axis=0)\n",
    "x_val_merged = tf.concat([X_val, Xs_val], axis=0)\n",
    "\n",
    "y_train_merged = tf.concat([Y_train, Ys_train], axis=0)\n",
    "y_test_merged = tf.concat([Y_test, Ys_test], axis=0)\n",
    "y_val_merged = tf.concat([Y_val, Ys_val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([101465, 50, 50, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two: Build the CNN Model \n",
    "Referencing the CNN model described in the [Using Deep Convolutional Networks for Gesture Recognition in American Sign Language](https://arxiv.org/pdf/1710.06836.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), input_shape=(img_size, img_size, 3)),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3)),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Conv2D(32, (3, 3)),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3)),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Conv2D(64, (3, 3)),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3)),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128),\n",
    "        layers.Activation(activations.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(37, activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "793/793 [==============================] - 803s 1s/step - loss: 1.3244 - accuracy: 0.6057 - val_loss: 0.3493 - val_accuracy: 0.8722\n",
      "Epoch 2/50\n",
      "793/793 [==============================] - 778s 980ms/step - loss: 0.3910 - accuracy: 0.8667 - val_loss: 0.1367 - val_accuracy: 0.9516\n",
      "Epoch 3/50\n",
      "793/793 [==============================] - 714s 900ms/step - loss: 0.2452 - accuracy: 0.9127 - val_loss: 0.1390 - val_accuracy: 0.9457\n",
      "Epoch 4/50\n",
      "793/793 [==============================] - 785s 990ms/step - loss: 0.1809 - accuracy: 0.9346 - val_loss: 0.0833 - val_accuracy: 0.9680\n",
      "Epoch 5/50\n",
      "793/793 [==============================] - 809s 1s/step - loss: 0.1484 - accuracy: 0.9466 - val_loss: 0.0821 - val_accuracy: 0.9696\n",
      "Epoch 6/50\n",
      "793/793 [==============================] - 729s 920ms/step - loss: 0.1258 - accuracy: 0.9547 - val_loss: 0.5398 - val_accuracy: 0.8476\n",
      "Epoch 7/50\n",
      "793/793 [==============================] - 739s 932ms/step - loss: 0.1101 - accuracy: 0.9601 - val_loss: 0.0633 - val_accuracy: 0.9761\n",
      "Epoch 8/50\n",
      "793/793 [==============================] - 791s 997ms/step - loss: 0.1005 - accuracy: 0.9641 - val_loss: 0.0853 - val_accuracy: 0.9688\n",
      "Epoch 9/50\n",
      "793/793 [==============================] - 757s 955ms/step - loss: 0.0837 - accuracy: 0.9697 - val_loss: 0.1041 - val_accuracy: 0.9624\n",
      "Epoch 10/50\n",
      "793/793 [==============================] - 758s 956ms/step - loss: 0.0791 - accuracy: 0.9718 - val_loss: 0.0462 - val_accuracy: 0.9833\n",
      "Epoch 11/50\n",
      "793/793 [==============================] - 671s 847ms/step - loss: 0.0718 - accuracy: 0.9745 - val_loss: 0.0909 - val_accuracy: 0.9682\n",
      "Epoch 12/50\n",
      "793/793 [==============================] - 663s 837ms/step - loss: 0.0673 - accuracy: 0.9766 - val_loss: 0.0268 - val_accuracy: 0.9903\n",
      "Epoch 13/50\n",
      "793/793 [==============================] - 652s 822ms/step - loss: 0.0638 - accuracy: 0.9774 - val_loss: 0.0239 - val_accuracy: 0.9917\n",
      "Epoch 14/50\n",
      "793/793 [==============================] - 648s 817ms/step - loss: 0.0586 - accuracy: 0.9794 - val_loss: 0.0304 - val_accuracy: 0.9886\n",
      "Epoch 15/50\n",
      "793/793 [==============================] - 658s 829ms/step - loss: 0.0554 - accuracy: 0.9808 - val_loss: 0.5919 - val_accuracy: 0.8502\n",
      "Epoch 16/50\n",
      "793/793 [==============================] - 662s 834ms/step - loss: 0.0521 - accuracy: 0.9820 - val_loss: 0.0170 - val_accuracy: 0.9950\n",
      "Epoch 17/50\n",
      "793/793 [==============================] - 658s 829ms/step - loss: 0.0477 - accuracy: 0.9837 - val_loss: 0.0583 - val_accuracy: 0.9817\n",
      "Epoch 18/50\n",
      "793/793 [==============================] - 651s 821ms/step - loss: 0.0463 - accuracy: 0.9842 - val_loss: 0.0112 - val_accuracy: 0.9957\n",
      "Epoch 19/50\n",
      "793/793 [==============================] - 653s 824ms/step - loss: 0.0463 - accuracy: 0.9843 - val_loss: 0.0390 - val_accuracy: 0.9862\n",
      "Epoch 20/50\n",
      "793/793 [==============================] - 643s 811ms/step - loss: 0.0412 - accuracy: 0.9856 - val_loss: 0.0526 - val_accuracy: 0.9818\n",
      "Epoch 21/50\n",
      "793/793 [==============================] - 674s 849ms/step - loss: 0.0397 - accuracy: 0.9865 - val_loss: 0.0105 - val_accuracy: 0.9960\n",
      "Epoch 22/50\n",
      "793/793 [==============================] - 787s 993ms/step - loss: 0.0383 - accuracy: 0.9869 - val_loss: 0.0151 - val_accuracy: 0.9955\n",
      "Epoch 23/50\n",
      "793/793 [==============================] - 728s 918ms/step - loss: 0.0376 - accuracy: 0.9871 - val_loss: 0.0086 - val_accuracy: 0.9967\n",
      "Epoch 24/50\n",
      "793/793 [==============================] - 715s 902ms/step - loss: 0.0371 - accuracy: 0.9876 - val_loss: 0.0104 - val_accuracy: 0.9963\n",
      "Epoch 25/50\n",
      "793/793 [==============================] - 721s 909ms/step - loss: 0.0340 - accuracy: 0.9883 - val_loss: 0.0094 - val_accuracy: 0.9967\n",
      "Epoch 26/50\n",
      "793/793 [==============================] - 777s 980ms/step - loss: 0.0322 - accuracy: 0.9890 - val_loss: 0.0237 - val_accuracy: 0.9916\n",
      "Epoch 27/50\n",
      "793/793 [==============================] - 707s 892ms/step - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.0074 - val_accuracy: 0.9974\n",
      "Epoch 28/50\n",
      "793/793 [==============================] - 766s 966ms/step - loss: 0.0307 - accuracy: 0.9896 - val_loss: 0.0073 - val_accuracy: 0.9976\n",
      "Epoch 29/50\n",
      "793/793 [==============================] - 753s 949ms/step - loss: 0.0318 - accuracy: 0.9892 - val_loss: 0.0076 - val_accuracy: 0.9974\n",
      "Epoch 30/50\n",
      "793/793 [==============================] - 696s 877ms/step - loss: 0.0297 - accuracy: 0.9901 - val_loss: 0.0068 - val_accuracy: 0.9974\n",
      "Epoch 31/50\n",
      "793/793 [==============================] - 754s 951ms/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 0.0129 - val_accuracy: 0.9950\n",
      "Epoch 32/50\n",
      "793/793 [==============================] - 773s 975ms/step - loss: 0.0280 - accuracy: 0.9906 - val_loss: 0.0051 - val_accuracy: 0.9983\n",
      "Epoch 33/50\n",
      "793/793 [==============================] - 751s 946ms/step - loss: 0.0278 - accuracy: 0.9908 - val_loss: 0.0094 - val_accuracy: 0.9968\n",
      "Epoch 34/50\n",
      "793/793 [==============================] - 713s 899ms/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
      "Epoch 35/50\n",
      "793/793 [==============================] - 802s 1s/step - loss: 0.0268 - accuracy: 0.9908 - val_loss: 0.0104 - val_accuracy: 0.9961\n",
      "Epoch 36/50\n",
      "793/793 [==============================] - 765s 965ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.0067 - val_accuracy: 0.9975\n",
      "Epoch 37/50\n",
      "793/793 [==============================] - 842s 1s/step - loss: 0.0248 - accuracy: 0.9915 - val_loss: 0.0055 - val_accuracy: 0.9981\n",
      "Epoch 38/50\n",
      "793/793 [==============================] - 705s 888ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.0043 - val_accuracy: 0.9984\n",
      "Epoch 39/50\n",
      "793/793 [==============================] - 842s 1s/step - loss: 0.0245 - accuracy: 0.9919 - val_loss: 0.0302 - val_accuracy: 0.9911\n",
      "Epoch 40/50\n",
      "793/793 [==============================] - 764s 964ms/step - loss: 0.0219 - accuracy: 0.9925 - val_loss: 0.0043 - val_accuracy: 0.9985\n",
      "Epoch 41/50\n",
      "793/793 [==============================] - 707s 892ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.0019 - val_accuracy: 0.9995\n",
      "Epoch 42/50\n",
      "793/793 [==============================] - 794s 1s/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 0.0020 - val_accuracy: 0.9994\n",
      "Epoch 43/50\n",
      "793/793 [==============================] - 814s 1s/step - loss: 0.0210 - accuracy: 0.9931 - val_loss: 0.0061 - val_accuracy: 0.9980\n",
      "Epoch 44/50\n",
      "793/793 [==============================] - 803s 1s/step - loss: 0.0214 - accuracy: 0.9930 - val_loss: 0.3878 - val_accuracy: 0.9204\n",
      "Epoch 45/50\n",
      "793/793 [==============================] - 789s 995ms/step - loss: 0.0207 - accuracy: 0.9928 - val_loss: 0.0044 - val_accuracy: 0.9983\n",
      "Epoch 46/50\n",
      "793/793 [==============================] - 750s 946ms/step - loss: 0.0202 - accuracy: 0.9934 - val_loss: 0.0278 - val_accuracy: 0.9912\n",
      "Epoch 47/50\n",
      "793/793 [==============================] - 770s 971ms/step - loss: 0.0200 - accuracy: 0.9934 - val_loss: 0.0042 - val_accuracy: 0.9982\n",
      "Epoch 48/50\n",
      "793/793 [==============================] - 834s 1s/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 0.0044 - val_accuracy: 0.9984\n",
      "Epoch 49/50\n",
      "793/793 [==============================] - 770s 969ms/step - loss: 0.0191 - accuracy: 0.9934 - val_loss: 0.0067 - val_accuracy: 0.9976\n",
      "Epoch 50/50\n",
      "793/793 [==============================] - 726s 915ms/step - loss: 0.0176 - accuracy: 0.9941 - val_loss: 0.0110 - val_accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_merged, y_train_merged, batch_size=128, epochs=50, validation_data = (x_val_merged,y_val_merged),shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: merged dataset with aug/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"merged dataset with aug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the history for plotting:\n",
    "f = open('history_merged_data_withaug.pckl', 'wb')\n",
    "pickle.dump(history.history, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_aug_model = keras.models.load_model(\"dataset without aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 5s 29ms/step - loss: 0.0233 - accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "no_aug_scores = no_aug_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model = keras.models.load_model(\"dropout batchnorm with aug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 7s 41ms/step - loss: 0.0142 - accuracy: 0.9965\n"
     ]
    }
   ],
   "source": [
    "aug_scores = aug_model.evaluate(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 4s 26ms/step - loss: 0.0448 - accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "aug_nbn_model = keras.models.load_model(\"dropout with aug\")\n",
    "aug_nbn_scores = aug_nbn_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
